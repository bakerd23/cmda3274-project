---
title: "Context-Aware 2K-Style Ratings for NCAA Basketball"
author: Baker Dean & Caleb Ramsey
output: pdf_document
---

```{r, include=FALSE}
library(dplyr)
library(ggplot2)
library(randomForest)
library(knitr)
mbb_raw <- read.csv("Data/bt2024.csv")
wbb_raw <- read.csv("Data/wcbb_players_2025_D1.csv")
men_4099 <- read.csv("Data/men_4099.csv")
women_4099 <- read.csv("Data/women_4099.csv")
ratings_23 <- read.csv("Data/ratings_23.csv")
ratings_24 <- read.csv("Data/ratings_24.csv")
hdi23 <- read.csv("Data/HDI Ratings 2023-24.csv")
hdi24 <- read.csv("Data/HDI Ratings 2024-25.csv")
wcbb_final_ratings = read.csv("Data/wcbb_final_ratings.csv")
conf_strength_named = read.csv("Data/mbb_conf_strength_named.csv")
```

# Introduction

## Research Question

How can we create a player rating system for men’s and women’s college basketball that accounts for both player performance and conference strength, allowing staff to compare athletes across divisions and evaluate transfer portal talent more effectively?

## Abstract

Player evaluation has become more complex than ever in this era of transfer portal and Name, Image, and Likeness (NIL) rules. Player movement is at an all-time high with teams often turning over more than 75% of their teams every offseason and with players able to be paid for their services coaches and general managers must be aware of the true value of a player, how much to offer him/her and how to allocate all their financial resources. With many players choosing to move between the Power 5 conferences (ACC, Big Ten, Big 12, Big East, and SEC) and the many smaller conferences, evaluating the skill and fit of a player is not as simple as simply pulling ups stats and sorting by points per game. With such a wide variety of levels of talent, determining how to account for the level of competition they faced becomes an important factor alongside the performance of the player.

This project develops a video-game style player ratings system that combines box-score performance with measures of conference strength and schedule difficulty. We use existing HDI ratings as a baseline model for men's data, then extend,scale, and adapt that framework to the women's game. After constructing conference-level strength metrics, we apply adjustments that account for opponent difficulty and competitive environment. Our long-term goal is a transferable rating system that supports coaching staffs in scouting, rescruiting, and evaluating transfer-portal talent.

# Methods
![Pipeline Diagram](Data/Pipeline_Diagram.png)

## Data Collection

### Men's

Men's data was collected through two primary sources:

1\.
HDI ratings & player statistics, accessed through a Google Sheet provided by Virginia Tech Men's Basketball staff and downloaded as a CSV.

2\.
Barttorvik statistics, scraped using the `cbbdata` package (Weatherman, 2024) for the corresponding seasons.

The two datasets were joined using a custom matching function designed to handle inconsistencies in naming and duplicate names across teams.
Players appearing in only one dataset or with incomplete records were removed.

```{r, include=FALSE}
model_data_printing <- read.csv("Data/ratings_24.csv") %>% select(
  mpg, ppg, rpg, apg, spg, bpg, two_pct, three_pct, ft_pct, tov, ast_to, efg, ts, Rating) %>% mutate(
    mpg = round(mpg),
    ppg = round(ppg,1),
    rpg = round(rpg,1),
    apg = round(apg,1),
    spg = round(spg,1),
    bpg = round(bpg,1),
    two_pct = round(two_pct*100,1),
    three_pct = round(three_pct*100,1),
    ft_pct = round(ft_pct*100,1),
    tov = round(tov,1),
    ast_to = round(ast_to,2),
    ts = round(ts,1)) %>% rename(
      `2%` = two_pct,
      `3%` = three_pct,
      `ft%` = ft_pct,
      Rtg = Rating,
      a_to = ast_to)
```
```{r, include=FALSE}
kable(head(model_data_printing,5))
```
```{r, include=FALSE}
mbb_raw_printing <- mbb_raw %>% select(
  Name, mpg, ppg, rpg, apg, spg, bpg, two_pct, three_pct, ft_pct, tov, ast_to, efg, ts) %>% mutate(
    mpg = round(mpg),
    ppg = round(ppg,1),
    rpg = round(rpg,1),
    apg = round(apg,1),
    spg = round(spg,1),
    bpg = round(bpg,1),
    two_pct = round(two_pct*100,1),
    three_pct = round(three_pct*100,1),
    ft_pct = round(ft_pct*100,1),
    tov = round(tov,1),
    ast_to = round(ast_to,2),
    ts = round(ts,1)) %>% rename(
      `2%` = two_pct,
      `3%` = three_pct,
      `ft%` = ft_pct,
      a_to = ast_to) %>%
  arrange(Name)
```
```{r, include = FALSE}
kable(head(mbb_raw_printing,5))
```

We use these cleaned data as the basis for the men’s rating model.

### Women's

Women's data were collected from ESPN via the `wehoop` package (Gilani & Hutchinson, 2021):
* `load_wbb_player_box()` provided game-level box scores for all players in the 2025 season.
* `load_wbb_schedule` provided team IDs, opponents, and conference fields used in conference assignment.
We aggregated player box scores to season totals and standardized minutes, shooting percentages, and efficiency metrics. Teams with ambiguous conference IDs were resolved manually. We restricted the dataset to Division I teams and enforced a minimum threshold (Minutes = 200, Games Played = 10). 

```{r, include=FALSE}
wbb_raw_printing <- wbb_raw %>% mutate(
  mpg = minutes / games_played,
  ppg = points / games_played,
  rpg = rebounds / games_played,
  apg = assists / games_played,
  spg = steals / games_played,
  bpg = blocks / games_played,
  tov = turnovers / games_played,
  two_fg_made = field_goals_made - three_point_made,
  two_fg_att  = field_goals_attempted - three_point_attempted,
  two_pct = ifelse(two_fg_att > 0, two_fg_made / two_fg_att, NA_real_),
  ast_to = ifelse(turnovers > 0, assists / turnovers, NA_real_),
  efg = ifelse(field_goals_attempted > 0, (field_goals_made + 0.5 * three_point_made) / field_goals_attempted, NA_real_),
  ts = ifelse(field_goals_attempted + 0.44 * free_throws_attempted > 0, points / (2 * (field_goals_attempted + 0.44 * free_throws_attempted)), NA_real_)) %>% select(
    player_name, team_name, mpg, ppg, rpg, apg, spg, bpg, two_pct, three_point_percent, free_throw_percent, tov, ast_to, efg, ts) %>% mutate(
      mpg = round(mpg),
      ppg = round(ppg,1),
      rpg = round(rpg,1),
      apg = round(apg,1),
      spg = round(spg,1),
      bpg = round(bpg,1),
      two_pct = round(two_pct*100,1),
      three_point_percent = round(three_point_percent*100,1),
      free_throw_percent = round(free_throw_percent*100,1),
      tov = round(tov,1),
      ast_to = round(ast_to,2),
      ts = round(ts*100,1),
      efg = round(efg*100,1)) %>% rename(
      `2%` = two_pct,
      `3%` = three_point_percent,
      `ft%` = free_throw_percent,
      a_to = ast_to) %>%
  na.omit() %>%
  select(-team_name) %>%
  arrange(player_name)
```
```{r, include = FALSE}
kable(head(wbb_raw_printing,5))
```

These processed data serve as the basis for the women’s rating model.

## Data Manipulation

### Men's Statistical Model

We trained a random forest model to predict HDI ratings using per-game and efficiency statistics, including:

* points, rebounds, assists, steals, and blocks per game (ppg, rpg, apg, spg, and bpg)
* shooting efficiency (2P%, 3P%, FT%)
* effective field goal percentage (eFG%)
* true shooting percentage (TS%)
* turnovers and assist-to-turnover ratio (A/TO)
* free-throw rate (FTR)

This model produces a statistics score that captures each player’s individual production independent of conference effects. We apply this model to both the men's and women's datasets.

```{r, include = FALSE}
kable(head(men_4099,5))
```

### Women's Role-Adjusted Model

ESPN’s women’s data do not include reliable positional labels, so we created three role buckets: guard, wing, center. These buckets use simple rules based on height, assist rate, rebounding profile, and shooting tendencies. The goal was not perfect positional classification, but to ensure players were evaluated relative to the expectations of their role. For each player we computed per-game and advanced metrics (TS%, eFG%, usage proxies) and converted them into percentile scores. We then combined these percentiles with role-specific weights. Finally, we integrated team strength, opponent strength-of-schedule, and conference strength to produce a composite rating scaled to 40–99.

```{r, include = FALSE}
kable(head(women_4099,5))
```

## Conference Strength Modeling

### Men's 

For men's basketball, we estimated team strength directly from scoring margins using ESPN game results via the hoopR::load_mbb_schedule() function. We restricted to D1 matchups (conf IDs: 1-32) and constructed a matrix where each row encoded a game, with a +1 for the home team, -1 for the away team, andd a shared intercept column. We then solved a least-squares system: 

$$(A^T(A) + \lambda(I))\beta = A^T(y)$$

where y is the home-away score margin and $\lambda = 10^{-2}$ is a small penalty. The resulting coefficients were centered with mean zero and then standardized to a z-score, yielding a latent team strength measure (team_z) for each team. 

To quantify strength-of-schedule (SOS), we paired each game with its opponent and averaged opponent team_z values weighted by a small home/away factor (0.9 for a home games and 1.1 for road games). This produced a raw SOS measure for each team, which we standardized to sos_z. Finally, we aggregated to the conference level by averaging team_z and sos_z within each conference and converting these conference means to z-scores (conf_strength_z and conf_sos_z). These two standardized conference-level quantities form inputs to the conference index.

We combined conference-level strength and SOS into a single conference index: ConfIndex = 0.5(Z_strength) + 0.5(Z_sos)
We then used that for our conference adjustment rating: FinalRating = BaseRating + w(ConfIndex) where w is a tuning parameter controlling adjustment strength

### Women's

For women's basketball, we constructed team and conference strength using ESPN WBB schedules and final scores. From the schedule data, we built a game-level table with home/away team IDs and final scores, then computed score margin as home points minus away points. Using this margin data, we fit a least squares model with +1 for the home team, -1 for the away team, and a shared intercept term, mirroring the men's approach. The resulting team coefficients were centered around mean zero and standardized to obtain a team-strength z-score for each team (team_z_map).

We then computed strength-of-schedule (SOS) by pairing each game with its opponent and averaging opponent team-strength z-scores, using a weight of 0.9 for home games and 1.1 for road games to reflect the added difficulty of road atmospheres. This produced a raw SOS value per team, which we standardized to sos_z. To move from teams to conferences, we linked each team to a conference_name from the player table, averaged team-strength values within each conference, and standardized those conference means to obtain a conference-strength z-score conf_z. Finally, we merged team_z, sos_z, and conf_z back into the women's player table (players_enriched_avg) and flagged Power 5 teams (conf_tier_pow5) so that team strength, schedule, and conference tier could all be incorporated into the final women's rating formula. 

```{r, message=FALSE, warning=FALSE, include=FALSE}
conf_men = conf_strength_named %>%
  transmute(
    gender = "Men",
    conference = as.factor(conference_id),
    conf_strength_z = conf_strength_z,
    conf_sos_z = conf_sos_z
    )

conf_women = wcbb_final_ratings %>%
  group_by(conf) %>%
  summarise(
    avg_team_z = mean(team_z, na.rm = T),
    avg_sos_z = mean(sos_z, na.rm = T),
    .groups = "drop"
    ) %>%
  mutate(
    conf_strength_z = as.numeric(scale(avg_team_z)),
    conf_sos_z = as.numeric(scale(avg_sos_z))
    ) %>%
  transmute(
    gender = "Women",
    conference = conf,
    conf_strength_z,
    conf_sos_z
    )

conf_both = bind_rows(conf_men, conf_women)

conf_scatterplot = ggplot(conf_both, aes(x = conf_strength_z, 
                                  y = conf_sos_z, 
                                  color = gender)) +
  geom_hline(yintercept = 0, linewidth = 0.3) +
  geom_vline(xintercept = 0, linewidth = 0.3) +
  geom_point(size = 2, alpha = 0.8) +
  labs(
    title = "Conference Strength vs.\n Strength of Schedule",
    x = "Conference strength (z-score)",
    y = "Conference SOS (z-score)",
    color = "Gender"
    ) +
  theme_minimal(base_size = 11)

conf_hist = ggplot(conf_both, aes(x = conf_strength_z)) +
  geom_histogram(bins = 10) +
  facet_wrap(~ gender, ncol = 1) +
  labs(
    title = "Distribution of Conference Strength\n (z-scores) by Gender",
    x = "Conference strength (z-score)",
    y = "Count"
    ) + theme_minimal(base_size = 11)
```
```{r, fig.width=4, fig.height=3}
conf_scatterplot
conf_hist
```

The histograms indicate that men’s conferences vary more widely in strength, while women’s conferences are more tightly clustered with only a few elite outliers. 

The scatterplot confirms a strong positive relationship between conference strength and schedule difficulty for both genders. Stronger conferences consistently play harder schedules, while weaker conferences play softer ones. 

These findings demonstrate that conference environment has a meaningful impact on player's stats and supports the need for conference-adjusted ratings in our model.

# Analysis

To evaluate how well our statistical model replicates the original HDI ratings, we compared our model's predictions against the actual HDI ratings for the men's basketball players:

```{r, message=FALSE, warning=FALSE, include=FALSE}
hdi23_simple <- hdi23 %>%
  rename(Name = Full.Name, Team = X2023.2024.School) %>%
  select(Name, Team, Rating)

hdi24_simple <- hdi24 %>%
  rename(Name = Full.Name, Team = X2024.2025.School) %>%
  select(Name, Team, Rating)

ratings <- rbind(ratings_23, ratings_24)
ratings_model_data <- ratings %>% 
  select(mpg, ppg, rpg, apg, spg, bpg, two_pct, three_pct, ft_pct, 
         tov, ast_to, efg, ts, ftr, Rating) %>% 
  na.omit()

set.seed(3274)
index <- sample(nrow(ratings_model_data), size = floor(0.8*nrow(ratings_model_data)))
train <- ratings_model_data[index, ]
test  <- ratings_model_data[-index, ]
ratings_model <- randomForest(
  Rating ~ mpg + ppg + rpg + apg + spg + bpg + two_pct + three_pct + 
           ft_pct + tov + ast_to + efg + ts + ftr,
  data = train, ntree = 1000, importance = TRUE
)

ratings_24_comparison <- ratings_24 %>%
  select(Name, Team, mpg, ppg, rpg, apg, spg, bpg, two_pct, three_pct, 
         ft_pct, tov, ast_to, efg, ts, ftr, Rating) %>%
  na.omit()

ratings_24_comparison$predicted_rating <- predict(ratings_model, newdata = ratings_24_comparison)

ratings_24_comparison <- ratings_24_comparison %>%
  mutate(
    Our_Rating = 40 + (predicted_rating - min(predicted_rating)) * 
                  (59 / (max(predicted_rating) - min(predicted_rating)))
  ) %>%
  rename(HDI_Rating = Rating)

cor_value <- cor(ratings_24_comparison$HDI_Rating, 
                 ratings_24_comparison$Our_Rating, 
                 use = "complete.obs")
rmse_value <- sqrt(mean((ratings_24_comparison$HDI_Rating - 
                        ratings_24_comparison$Our_Rating)^2, na.rm = TRUE))
mean_diff <- mean(ratings_24_comparison$Our_Rating - 
                  ratings_24_comparison$HDI_Rating, na.rm = TRUE)

plot <- ggplot(ratings_24_comparison, aes(x = HDI_Rating, y = Our_Rating)) +
  geom_point(alpha = 0.4, size = 2, color = "steelblue") +
  annotate("text", x = 50, y = 95, 
           label = paste0("r^2: ", round(cor_value^2, 3)), 
           hjust = 0, size = 4, fontface = "bold") +
  labs(
    title = "HDI vs. Stats Rating 2024-25",
    x = "HDI Rating",
    y = "Our Statistical Rating",
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.title = element_text(size = 11),
    plot.caption = element_text(size = 9, hjust = 0)
  ) +
  coord_fixed(ratio = 1, xlim = c(40, 100), ylim = c(40, 100))
```
```{r, fig.width=5, fig.height=4}
plot
```

Since our ratings were scaled to be 40-99 (similar to NBA2K, a basketball video game) and have not been adjusted for conference weight yet, this shape is perfectly reasonable and an indication that the statistical portion of the code is working. There are only a few outliers and the narrowing of the graph at the top is to be expected given a fewer number of elite players. A 91% r^2 value indicates that the stats we used in the model account for 91% of the variability in the ratings which is a great report given that we could not scrape many of the more advanced statistics for the women's players.

## Potential Limitations (so far)

### Mens to womens model translation

One of the problems we ran into was applying a model trained on men's data to the women's game, the original outputs were too low (top players were in the 70s) due to the different paces and styles so this led us to scaling the ratings. 

### Adjusting for conferences on HDI ratings

As we have begun to work on adjusting for conferences on the HDI ratings it is difficult to bring down top mid-major players without destroying the middle tier players from lower conferences so we are still working on a way to balance this out.

```{r, include=FALSE}
# Unreliable WBB positional data
# ESPN's women's data do not include reliable position labels, so we created role buckets (guard, wing, center) using rebounding profile, assist-rate, and shooting distribution. This rule-based approach is practical but imperfect; misclassification can affect individual ratings, especially for versatile players. 
```

### Heavy reliance on box-score stats

Both the men and women's models are constrained to box-score variables because play-by-play and advanced tracking data are unavailable for the full dataset. This limits the model's ability to capture defensive impact, screening, spacing, off-ball value, and other impacts players may have that aren't captured in the traditional box-score. 

### Margin-based team strength models assume linearity

Our team strength estimates rely on least-squares systems that use scoring margins. Margin-based ratings assume linear, symmetric behavior and may overweight blowouts or undervalue teams that eek out close games, especially close wins versus good teams. 

### Small conferences harder to adjust for

Conferences with fewer teams or limited non-conference scheduling leads to unstable conference z-scores that make estimating their strength less reliable.

### Transferability to future seasons untested

The model is tuned and validated on the 2024-25 season. It's stable across single seasons, but tests across multiple seasons, roster turnover patterns, and NIL-induced shifts are not yet validated.

# References


Weatherman A (2024). `cbbdata`: API for College Basketball Data. R package version 0.3.0.9000, https://cbbdata.aweatherman.com/

Kartes, Weston. MBB Player Database 2024. HD Intelligence. https://www.hdintelligence.com/mbb-player-database-2024 

Kartes, Weston. MBB Player Database 2025. HD Intelligence. https://www.hdintelligence.com/mbb-player-database-2025 

Torvik, Bart. NCAAM Player Stats 2024. barttorvik. https://barttorvik.com/playerstat.php?year=2024 

Torvik, Bart. NCAAM Player Stats 2025. barttorvik. https://barttorvik.com/playerstat.php?year=2025 

Gilani & Hutchinson. `wehoop`: Women's Basketball Data for R: https://github.com/sportsdataverse/wehoop

