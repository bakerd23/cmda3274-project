---
title: "Context-Aware 2K-Style Ratings for NCAA Basketball"
author: Baker Dean & Caleb Ramsey
output: 
  pdf_document: default
header-includes:
  - \usepackage{float}
---

```{r, include=FALSE}
library(dplyr)
library(ggplot2)
library(randomForest)
library(knitr)
mbb_raw <- read.csv("Data/bt2024.csv")
wbb_raw <- read.csv("Data/wcbb_players_2025_D1.csv")
men_4099 <- read.csv("Data/men_4099.csv")
women_4099 <- read.csv("Data/women_4099.csv")
ratings_23 <- read.csv("Data/ratings_23.csv")
ratings_24 <- read.csv("Data/ratings_24.csv")
hdi23 <- read.csv("Data/HDI Ratings 2023-24.csv")
hdi24 <- read.csv("Data/HDI Ratings 2024-25.csv")
wcbb_final_ratings = read.csv("Data/wcbb_final_ratings.csv")
conf_strength_named = read.csv("Data/mbb_conf_strength_named.csv")
```

# Introduction

## Research Question

How can we create a player rating system for men’s and women’s college basketball that accounts for both player performance and conference strength, allowing staff to compare athletes across divisions and evaluate transfer portal talent more effectively?

## Abstract

Player evaluation has become more complex than ever in this era of transfer portal and Name, Image, and Likeness (NIL) rules. Player movement is at an all-time high with teams often turning over more than 75% of their teams every offseason and with players able to be paid for their services coaches and general managers must be aware of the true value of a player, how much to offer him/her and how to allocate all their financial resources. With many players choosing to move between the Power 5 conferences (ACC, Big Ten, Big 12, Big East, and SEC) and the many smaller conferences, evaluating the skill and fit of a player is not as simple as simply pulling up stats and sorting by points per game. With such a wide variety of levels of talent, determining how to account for the level of competition they faced becomes an important factor alongside the performance of the player.

This project develops a video-game style player ratings system that combines box-score performance with measures of conference strength and schedule difficulty. We use existing HDI ratings (Kartes, 2024) as a baseline model for men's data, then extend,scale, and adapt that framework to the women's game. After constructing conference-level strength metrics, we apply adjustments that account for opponent difficulty and competitive environment. Our long-term goal is a transferable rating system that supports coaching staffs in scouting, recruiting, and evaluating transfer-portal talent.

# Methods
```{r fig.cap = "Figure 1. Pipeline diagram", out.width = "80%"}
include_graphics("Data/Pipeline_Diagram.png")
```

Figure 1 illustrates the pipeline used in this project: data collection, cleaning, modeling, and conference adjustment.

## Data Collection

### Men's

Men's data was collected through two primary sources:

1\.
HDI ratings & player statistics, accessed through a Google Sheet provided by Virginia Tech Men's Basketball staff and downloaded as a CSV.

2\.
Barttorvik statistics (Torvik, 2024), scraped using the `cbbdata` package (Weatherman, 2024) for the corresponding seasons.

The two datasets were joined using a custom matching function designed to handle inconsistencies in naming and duplicate names across teams. Players appearing in only one dataset or with incomplete records were removed. An example of the raw dataset can be seen in Table 1 and the cleaned dataset in Table 2.

```{r, include=FALSE}
mbb_raw_printing <- mbb_raw %>% select(
  Name, mpg, ppg, rpg, apg, spg, bpg, two_pct, three_pct, ft_pct, tov, ast_to, efg, ts) %>% mutate(
    mpg = round(mpg),
    ppg = round(ppg,1),
    rpg = round(rpg,1),
    apg = round(apg,1),
    spg = round(spg,1),
    bpg = round(bpg,1),
    two_pct = round(two_pct*100,1),
    three_pct = round(three_pct*100,1),
    ft_pct = round(ft_pct*100,1),
    tov = round(tov,1),
    ast_to = round(ast_to,2),
    ts = round(ts,1)) %>% rename(
      `2%` = two_pct,
      `3%` = three_pct,
      `ft%` = ft_pct,
      a_to = ast_to) %>%
  arrange(Name)
```
```{r, echo = FALSE}
kable(head(mbb_raw_printing,5), caption = "Example of raw men's model data")
```
```{r, include=FALSE}
model_data_printing <- read.csv("Data/ratings_24.csv") %>% select(
  mpg, ppg, rpg, apg, spg, bpg, two_pct, three_pct, ft_pct, tov, ast_to, efg, ts, Rating) %>% mutate(
    mpg = round(mpg),
    ppg = round(ppg,1),
    rpg = round(rpg,1),
    apg = round(apg,1),
    spg = round(spg,1),
    bpg = round(bpg,1),
    two_pct = round(two_pct*100,1),
    three_pct = round(three_pct*100,1),
    ft_pct = round(ft_pct*100,1),
    tov = round(tov,1),
    ast_to = round(ast_to,2),
    ts = round(ts,1)) %>% rename(
      `2%` = two_pct,
      `3%` = three_pct,
      `ft%` = ft_pct,
      Rtg = Rating,
      a_to = ast_to)
```
```{r, echo=FALSE}
kable(head(model_data_printing,5), caption = "Example of cleaned men's model data")
```

We use these cleaned data as the basis for the men’s rating model.

### Women's

Women's data were collected from ESPN via the `wehoop` package (Gilani & Hutchinson, 2021):
* `load_wbb_player_box()` provided game-level box scores for all players in the 2025 season.
* `load_wbb_schedule` provided team IDs, opponents, and conference fields used in conference assignment.
We aggregated player box scores to season totals and standardized minutes, shooting percentages, and efficiency metrics. Teams with ambiguous conference IDs were resolved manually. We restricted the dataset to Division I teams and enforced a minimum threshold (Minutes = 200, Games Played = 10). 

```{r, include=FALSE}
wbb_data_printing <- wcbb_final_ratings %>% select(
    player, team, mpg, ppg, rpg, apg, spg, bpg, efg, ts) %>% mutate(
      mpg = round(mpg),
      ppg = round(ppg,1),
      rpg = round(rpg,1),
      apg = round(apg,1),
      spg = round(spg,1),
      bpg = round(bpg,1),
      ts = round(ts*100,1),
      efg = round(efg*100,1)) %>%
  na.omit() %>%
  select(-team) %>%
  arrange(player)
```
```{r, echo = FALSE}
kable(head(wbb_data_printing,5), caption = "Example of cleaned women's model data")
```

These processed data serve as the basis for the women’s rating model.

## Data Manipulation

### Men's Statistical Model

We trained a random forest model to predict HDI ratings using per-game and efficiency statistics, including:

* points, rebounds, assists, steals, and blocks per game (ppg, rpg, apg, spg, and bpg)
* shooting efficiency (2P%, 3P%, FT%)
* effective field goal percentage (eFG%)
* true shooting percentage (TS%)
* turnovers and assist-to-turnover ratio (A/TO)
* free-throw rate (FTR)

This model produces a statistics score that captures each player’s individual production independent of conference effects. We apply this model to both the men's (Table 4) and women's (Table 5) datasets.

```{r, echo = FALSE}
kable(head(men_4099,5), caption = "Men's Data")
```

### Women's Role-Adjusted Model

ESPN’s women’s data do not include reliable positional labels, so we created three role buckets: guard, wing, center. These buckets use simple rules based on height, assist rate, rebounding profile, and shooting tendencies. The goal was not perfect positional classification, but to ensure players were evaluated relative to the expectations of their role. For each player we computed per-game and advanced metrics (TS%, eFG%, usage proxies) and converted them into percentile scores. We then combined these percentiles with role-specific weights. Finally, we integrated team strength, opponent strength-of-schedule, and conference strength to produce a composite rating scaled to 40–99.

```{r, echo = FALSE}
kable(head(women_4099,5), caption = "Women's Data")
```

# Motivation for Conference Adjustment

Initial outputs revealed clear inconsistencies: players from lower-tier D1 conferences appeared near the top of the ratings despite producing those numbers against comparatively weaker opponents. This confirmed the need for a conference-based adjustment to place players on comparable footing across the country.

## Conference Strength Modeling

### Men's 

Using game results from `hoopR::load_mbb_schedule()` (Gilani, 2024), we restricted to Division I matchups and constructed a least-squares system:

$$(A^T(A) + \lambda(I))\beta = A^T(y)$$

where y is the home-away score margin and $\lambda = 10^{-2}$ is a small penalty. The resulting coefficients were centered with mean zero and then standardized to a z-score, yielding a latent team strength measure (team_z) for each team. 

To quantify strength-of-schedule (sos_z), we averaged opponent team_z values with weights (0.9 home, 1.1 away). We then aggregated to the conference level and standardized conference means to obtain:
* conference strength (conf_strength_z)
* conference schedule difficulty (conf_sos_z)

These form a conference index that adjusts the final player ratings.

### Women's

We replicated the same least-squares approach for women’s schedules using `wehoop`:
* compute team_z from score margins
* compute sos_z from weighted opponent strength
* aggregate to conferences and standardize

We merged these values back onto the women’s player table and flagged Power-5 programs to incorporate conference tier effects.

```{r, message=FALSE, warning=FALSE, include=FALSE}
conf_men = conf_strength_named %>%
  transmute(
    gender = "Men",
    conference = as.factor(conference_id),
    conf_strength_z = conf_strength_z,
    conf_sos_z = conf_sos_z
    )

conf_women = wcbb_final_ratings %>%
  group_by(conf) %>%
  summarise(
    avg_team_z = mean(team_z, na.rm = T),
    avg_sos_z = mean(sos_z, na.rm = T),
    .groups = "drop"
    ) %>%
  mutate(
    conf_strength_z = as.numeric(scale(avg_team_z)),
    conf_sos_z = as.numeric(scale(avg_sos_z))
    ) %>%
  transmute(
    gender = "Women",
    conference = conf,
    conf_strength_z,
    conf_sos_z
    )

conf_both = bind_rows(conf_men, conf_women)

conf_scatterplot = ggplot(conf_both, aes(x = conf_strength_z, 
                                  y = conf_sos_z, 
                                  color = gender)) +
  geom_hline(yintercept = 0, linewidth = 0.3) +
  geom_vline(xintercept = 0, linewidth = 0.3) +
  geom_point(size = 2, alpha = 0.8) +
  labs(
    title = "Conference Strength vs.\n Strength of Schedule",
    x = "Conference strength (z-score)",
    y = "Conference SOS (z-score)",
    color = "Gender"
    ) +
  theme_minimal(base_size = 11)

conf_hist = ggplot(conf_both, aes(x = conf_strength_z)) +
  geom_histogram(bins = 10) +
  facet_wrap(~ gender, ncol = 1) +
  labs(
    title = "Distribution of Conference Strength\n (z-scores) by Gender",
    x = "Conference strength (z-score)",
    y = "Count"
    ) + theme_minimal(base_size = 11)
```
```{r, fig.width=4, fig.height=3}
conf_scatterplot
conf_hist
```

The histograms indicate that men’s conferences vary more widely in strength, while women’s conferences are more tightly clustered with only a few elite outliers. 

The scatterplot confirms a strong positive relationship between conference strength and schedule difficulty for both genders. Stronger conferences consistently play harder schedules, while weaker conferences play softer ones. 

These findings demonstrate that conference environment has a meaningful impact on player's stats and supports the need for conference-adjusted ratings in our model.

# Analysis

To evaluate how well our statistical model replicates the original HDI ratings, we compared our model's predictions against the actual HDI ratings for the men's basketball players:

```{r, message=FALSE, warning=FALSE, include=FALSE}
hdi23_simple <- hdi23 %>%
  rename(Name = Full.Name, Team = X2023.2024.School) %>%
  select(Name, Team, Rating)

hdi24_simple <- hdi24 %>%
  rename(Name = Full.Name, Team = X2024.2025.School) %>%
  select(Name, Team, Rating)

ratings <- rbind(ratings_23, ratings_24)
ratings_model_data <- ratings %>% 
  select(mpg, ppg, rpg, apg, spg, bpg, two_pct, three_pct, ft_pct, 
         tov, ast_to, efg, ts, ftr, Rating) %>% 
  na.omit()

set.seed(3274)
index <- sample(nrow(ratings_model_data), size = floor(0.8*nrow(ratings_model_data)))
train <- ratings_model_data[index, ]
test  <- ratings_model_data[-index, ]
ratings_model <- randomForest(
  Rating ~ mpg + ppg + rpg + apg + spg + bpg + two_pct + three_pct + 
           ft_pct + tov + ast_to + efg + ts + ftr,
  data = train, ntree = 1000, importance = TRUE
)

ratings_24_comparison <- ratings_24 %>%
  select(Name, Team, mpg, ppg, rpg, apg, spg, bpg, two_pct, three_pct, 
         ft_pct, tov, ast_to, efg, ts, ftr, Rating) %>%
  na.omit()

ratings_24_comparison$predicted_rating <- predict(ratings_model, newdata = ratings_24_comparison)

ratings_24_comparison <- ratings_24_comparison %>%
  mutate(
    Our_Rating = 40 + (predicted_rating - min(predicted_rating)) * 
                  (59 / (max(predicted_rating) - min(predicted_rating)))
  ) %>%
  rename(HDI_Rating = Rating)

cor_value <- cor(ratings_24_comparison$HDI_Rating, 
                 ratings_24_comparison$Our_Rating, 
                 use = "complete.obs")
rmse_value <- sqrt(mean((ratings_24_comparison$HDI_Rating - 
                        ratings_24_comparison$Our_Rating)^2, na.rm = TRUE))
mean_diff <- mean(ratings_24_comparison$Our_Rating - 
                  ratings_24_comparison$HDI_Rating, na.rm = TRUE)

plot <- ggplot(ratings_24_comparison, aes(x = HDI_Rating, y = Our_Rating)) +
  geom_point(alpha = 0.4, size = 2, color = "steelblue") +
  annotate("text", x = 50, y = 95, 
           label = paste0("r^2: ", round(cor_value^2, 3)), 
           hjust = 0, size = 4, fontface = "bold") +
  labs(
    title = "HDI vs. Stats Rating 2024-25",
    x = "HDI Rating",
    y = "Our Statistical Rating",
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11),
    axis.title = element_text(size = 11),
    plot.caption = element_text(size = 9, hjust = 0)
  ) +
  coord_fixed(ratio = 1, xlim = c(40, 100), ylim = c(40, 100))
```
```{r, fig.width=5, fig.height=4}
plot
```

Since our ratings were scaled to be 40-99 (similar to NBA2K, a basketball video game) and have not been adjusted for conference weight yet, this shape is perfectly reasonable and an indication that the statistical portion of the code is working. There are only a few outliers and the narrowing of the graph at the top is to be expected given a fewer number of elite players. A 91% r^2 value indicates that the stats we used in the model account for 91% of the variability in the ratings which is a great report given that we could not scrape many of the more advanced statistics for the women's players.

## Potential Limitations (so far)

### Men's-to-women's model translation

One of the problems we ran into was applying a model trained on men's data to the women's game, the original outputs were too low (top players were in the 70s) due to the different paces and styles so this led us to scaling the ratings. 

### Adjusting for conferences on HDI ratings

As we have begun to work on adjusting for conferences on the HDI ratings it is difficult to bring down top mid-major players without destroying the middle tier players from lower conferences so we are still working on a way to balance this out.

```{r, include=FALSE}
# Unreliable WBB positional data
# ESPN's women's data do not include reliable position labels, so we created role buckets (guard, wing, center) using rebounding profile, assist-rate, and shooting distribution. This rule-based approach is practical but imperfect; misclassification can affect individual ratings, especially for versatile players. 
```

### Heavy reliance on box-score stats

Both the men and women's models are constrained to box-score variables because play-by-play and advanced tracking data are unavailable for the full dataset. This limits the model's ability to capture defensive impact, screening, spacing, off-ball value, and other impacts players may have that aren't captured in the traditional box-score. 

### Margin-based team strength models assume linearity

Our team strength estimates rely on least-squares systems that use scoring margins. Margin-based ratings assume linear, symmetric behavior and may overweight blowouts or undervalue teams that eke out close games, especially close wins versus good teams. 

### Small conferences harder to adjust for

Conferences with fewer teams or limited non-conference scheduling leads to unstable conference z-scores that make estimating their strength less reliable.

### Transferability to future seasons untested

The model is tuned and validated on the 2024-25 season. It's stable across single seasons, but tests across multiple seasons, roster turnover patterns, and NIL-induced shifts are not yet validated.

# References

Weatherman A (2024). `cbbdata`: API for College Basketball Data. R package version 0.3.0.9000, https://cbbdata.aweatherman.com/

Kartes, Weston. MBB Player Database 2024. HD Intelligence. https://www.hdintelligence.com/mbb-player-database-2024 

Kartes, Weston. MBB Player Database 2025. HD Intelligence. https://www.hdintelligence.com/mbb-player-database-2025 

Torvik, Bart. NCAAM Player Stats 2024. barttorvik. https://barttorvik.com/playerstat.php?year=2024 

Torvik, Bart. NCAAM Player Stats 2025. barttorvik. https://barttorvik.com/playerstat.php?year=2025 

Gilani & Hutchinson. `wehoop`: Women's Basketball Data for R: https://github.com/sportsdataverse/wehoop

Gilani, S. `hoopr`: The SportsDataverse's R Package for Men's Basketball Data: https://hoopR.sportsdataverse.org




